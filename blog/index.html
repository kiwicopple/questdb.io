<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Blog · QuestDB</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Always on time"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Blog · QuestDB"/><meta property="og:type" content="website"/><meta property="og:url" content="https://questdb.io/"/><meta property="og:description" content="Always on time"/><meta property="og:image" content="https://questdb.io/img/favicon.png"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://questdb.io/img/favicon.png"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://questdb.io/blog/atom.xml" title="QuestDB Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://questdb.io/blog/feed.xml" title="QuestDB Blog RSS Feed"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700|IBM+Plex+Sans:300,400,600,700"/><script type="text/javascript" src="/js/gtmgmt.js"></script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-145747842-1"></script><script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"></script><script type="text/javascript" src="/js/fonts.js"></script><script type="text/javascript" src="/js/menu-hack.js"></script><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-block-buttons.js"></script><script type="text/javascript" src="/js/getstarted.js"></script><script type="text/javascript" src="/js/hotjar.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="blog"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/QuestDB_Logo.png" alt="QuestDB"/><h2 class="headerTitleWithLogo">QuestDB</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/" target="_self">Home</a></li><li class=""><a href="/getstarted" target="_self">Get QuestDB</a></li><li class=""><a href="/docs/documentationOverview" target="_self">Documentation</a></li><li class="siteNavGroupActive siteNavItemActive"><a href="/blog/" target="_self">Blog</a></li><li class=""><a href="/careers" target="_self">Careers</a></li><li class=""><a href="/about" target="_self">About</a></li><li class=""><a href="https://serieux-saucisson-79115.herokuapp.com/" target="_self">Join</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search..." title="Search..."/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Recent Posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Recent Posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/blog/2020/05/12/interesting-things-we-learned-about-sums">Interesting things we learned about sums</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/02/using-simd-to-aggregate-billions-of-rows-per-second">Using SIMD to aggregate billions of rows per second</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/15/interthread">The art of thread messaging</a></li><li class="navListItem"><a class="navItem" href="/blog/2019/12/19/lineprot">What makes QuestDB faster than InfluxDB</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="posts"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/05/12/interesting-things-we-learned-about-sums">Interesting things we learned about sums</a></h1><p class="post-meta">May 12, 2020</p><div class="authorBlock"><p class="post-authorName"><a target="_blank" rel="noreferrer noopener">Tancrede Collard</a></p></div></header><article class="post-content"><div><span><p><img src="/blog/assets/road-runner.png" alt="alt-text"></p>
<p>In the world of databases, benchmarking performance has always been the hottest
topic. Who is faster for data ingestion and
queries? About a month ago we announced a new release with SIMD aggregations on
<a href="https://news.ycombinator.com/item?id=22803504">HackerNews</a> and
<a href="https://www.reddit.com/r/programming/comments/fwlk0k/questdb_using_simd_to_aggregate_billions_of/">Reddit</a>.
Fast. But were those results accurate too?</p>
<p>Speed is not everything. Some of the feedback we have received pointed us toward
the accuracy of our results. This is something typically overlooked in the
space, but our sums turned out to be &quot;naive&quot;, with small errors for large
computations. By compounding a very small error over and over through a set of
operations, it can eventually become significant enough for people to start
worrying about it.</p>
<p>We then went on to include an accurate summation algorithm (such as &quot;Kahan&quot; and
&quot;Neumaier&quot; compensated sums). Now that we're doing the sums accurately, we
wanted to see how it affected performance. There is typically a trade-off
between speed and accuracy. However, by extracting even more performance out of
QuestDB (see below for how we did it), we managed to compute accurate sums as
fast as naive ones! Since comparisons to Clickhouse have been our most frequent
question, we have run the numbers and <a href="#comparison-with-clickhouse">here</a> is what we got, 2x faster for summing
1bn doubles will nulls.</p>
<p>All of this is included in our new
<a href="https://github.com/questdb/questdb/releases/tag/4.2.1">release 4.2.1</a></p>
<p>You can find our repository on <a href="https://github.com/questdb/questdb">GitHub</a>. All
your
<a href="https://github.com/questdb/questdb/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc">issues</a>,
<a href="https://github.com/questdb/questdb/pulls?q=is%3Apr+is%3Aopen+sort%3Aupdated-desc">pull-requests</a>
and <a href="https://github.com/questdb/questdb">stars</a> are welcome
<span class="emoji">🙂</span>.</p>
<h3><a class="anchor" aria-hidden="true" id="how-did-we-get-there-tl-dr"></a><a href="#how-did-we-get-there-tl-dr" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How did we get there? TL;DR</h3>
<p>We used prefetch and co-routines techniques to pull data from RAM to cache in
parallel with other CPU instructions. Our performance was previously limited by
memory bandwidth - using these techniques would address this and allow us to
compute accurate sums as fast as naive sums.</p>
<p>With the help of prefetch we implemented the fastest and most accurate summation
we have ever <a href="#comparison-with-clickhouse">tested</a> - 68ms over 1bn double values with nulls
(versus 139ms for Clickhouse). We believe this is a significant advance in terms
of performance for accurate summations, and will help developers handling
intensive computations with large datasets.</p>
<h3><a class="anchor" aria-hidden="true" id="contents"></a><a href="#contents" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contents</h3>
<ul>
<li>An <a href="#inaccurate-summation">introductory example</a> of the problem with summing
doubles.</li>
<li>A <a href="#float-representation-and-truncation-accuracy-loss">quick glance</a> at
floating points inaccuracies.</li>
<li>A <a href="#kahans-algorithm-for-compensated-summation">presentation</a> of the Kahan
algorithm.</li>
<li>Our <a href="#implementation-with-simd-instructions">compensated sum implementation</a>
using SIMD instructions.</li>
<li>A <a href="#comparison-with-clickhouse">benchmark versus Clickhouse</a> for naive and
accurate summation methods.</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="inaccurate-summation"></a><a href="#inaccurate-summation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Inaccurate summation?</h3>
<p>Before we dig in, some of you might wonder how an addition can be inaccurate as
opposed to simply right or wrong.</p>
<p>CPUs are poor at dealing with floating-point values. Arithmetics are almost
always wrong, with a worst-case error proportional to the number of operations
<code>n</code>. As floating-point operations are intransitive, the order in which you
perform them also has an impact on accuracy.</p>
<p>Here is an example:</p>
<pre><code class="hljs css language-java"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>{
    System.out.println(<span class="hljs-number">5.1</span>+<span class="hljs-number">9.2</span>);
}
</code></pre>
<p>We ask to add <code>5.1</code> to <code>9.2</code>. The result should be <code>14.3</code>, but we get the
following instead.</p>
<pre><code class="hljs"><span class="hljs-number">14.299999999999999</span>
</code></pre>
<p>It is a small difference (only <code>0.000000000000001</code>), but it is still wrong. To
make matters worse, this error can be compounded.</p>
<pre><code class="hljs css language-java"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>{
    <span class="hljs-keyword">double</span> a = <span class="hljs-number">5.1</span>+<span class="hljs-number">9.2</span>;
    <span class="hljs-keyword">double</span> b = a + <span class="hljs-number">3.5</span>;
    <span class="hljs-keyword">double</span> c = <span class="hljs-number">14.3</span> + <span class="hljs-number">3.5</span>;
    System.out.println(<span class="hljs-string">"The result is: "</span> + b);
    System.out.print(<span class="hljs-string">"But we expected: "</span> + c);
}
</code></pre>
<pre><code class="hljs"><span class="hljs-attr">The result is:</span> <span class="hljs-number">17.799999999999997</span>
<span class="hljs-attr">But we expected:</span> <span class="hljs-number">17.8</span>
</code></pre>
<p>The error has just grown to <code>0.000000000000003</code> and will keep on growing as we
add operations.</p>
<h3><a class="anchor" aria-hidden="true" id="float-representation-and-truncation-accuracy-loss"></a><a href="#float-representation-and-truncation-accuracy-loss" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Float representation and truncation accuracy loss</h3>
<p>Decimal numbers are not accurately stored. This is well documented already, for
example on
<a href="https://stackoverflow.com/questions/588004/is-floating-point-math-broken/588014#588014">StackOverlow</a>
or <a href="https://0.30000000000000004.com/">here</a>.</p>
<p>Consequently, operations on floating points will return inaccurate results. This
is not the only problem. Performing operations is also likely to introduce more
errors and to grow the total error over time. One such case is once the result
of an operation has to be truncated to fit the original format. Here is a
simplified example of the <strong>truncation</strong> that happens when adding floats of
different orders of magnitude.</p>
<blockquote>
<p>For the below example we will be using base 10 and expressing the exponent as
a number rather than a binary for sake of simplicity. We assume 5 significant
digits.</p>
</blockquote>
<p>We start with both our numbers expressed in scientific notation.</p>
<p><img src="/blog/assets/sum-1.png" alt="alt-text"></p>
<p>Let's expand into decimal notation and place them on a similar scale so all
digits fit.</p>
<p><img src="/blog/assets/sum-2.png" alt="alt-text"></p>
<p>Now, let us express this sum back as one number in scientific notation. We have
to <code>truncate</code> the result back to 5 significant digits.</p>
<p><img src="/blog/assets/sum-3.png" alt="alt-text"></p>
<p>The result is incorrect. In fact, it is as if we did not sum anything.</p>
<h3><a class="anchor" aria-hidden="true" id="kahans-algorithm-for-compensated-summation"></a><a href="#kahans-algorithm-for-compensated-summation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Kahan's algorithm for compensated summation</h3>
<p>Compensated sum maintains a sum of accumulated errors and uses it to attempt to
correct the (inaccurate) sum by the total error amount. It does so by trying to
adjust each new number by the total accumulated error.</p>
<p>The main Compensated summation algorithm is the
<a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm" target="_blank">Kahan</a>
sum. It runs in 4 steps:</p>
<ul>
<li>Subtract the <code>running error</code> from the new <code>number</code> to get the
<code>adjusted number</code>. If this is the first number, then the running error is 0.</li>
<li>Add the <code>adjusted number</code> to the <code>running total</code> and truncate to the number of
significant digits. This is the <code>truncated result</code>.</li>
<li>Calculate the <code>new running error</code> as
<code>(truncated result - running total) - adjusted number</code>.</li>
<li>Assign the <code>truncated result</code> as the new <code>running total</code>.</li>
</ul>
<p>This works because of addition transitivity rules.</p>
<h3><a class="anchor" aria-hidden="true" id="implementation-with-simd-instructions"></a><a href="#implementation-with-simd-instructions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implementation with SIMD instructions</h3>
<p>Now, the interesting bit! QuestDB implements the same 4-step algorithm as Kahan.
However, it uses vectorized instructions to make things a lot faster. The idea
came from Zach Bjornson who wrote about this on
<a href="http://blog.zachbjornson.com/2019/08/11/fast-float-summation.html" target="_blank">
his blog</a>.</p>
<p>Here is our implementation in details:</p>
<p>We first define our vectors:</p>
<pre><code class="hljs css language-java">Vec8d inputVec;
<span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> step = <span class="hljs-number">8</span>;
<span class="hljs-keyword">const</span> auto *lim = d + count;
<span class="hljs-keyword">const</span> auto remainder = (int32_t) (count - (count / step) * step);
<span class="hljs-keyword">const</span> auto *lim_vec = lim - remainder;
Vec8d sumVec = <span class="hljs-number">0</span>.;
Vec8d yVec;
Vec8d cVec = <span class="hljs-number">0</span>.;
Vec8db bVec;
Vec8q nancount = <span class="hljs-number">0</span>;
Vec8d tVec;
</code></pre>
<p>Then we load vectors with data. What's happening below is exactly Kahan's
algorithm. However, instead of summing individual values, we are summing vectors
of 8 values each.</p>
<pre><code class="hljs css language-java"><span class="hljs-keyword">for</span> (; d &lt; lim_vec; d += step) {
    _mm_prefetch(d + <span class="hljs-number">63</span> * step, _MM_HINT_T1);
    inputVec.load(d);
    bVec = is_nan(inputVec);
    nancount = if_add(bVec, nancount, <span class="hljs-number">1</span>);
    yVec = select(bVec, <span class="hljs-number">0</span>, inputVec - cVec);
    tVec = sumVec + yVec;
    cVec = (tVec - sumVec) - yVec;
    sumVec = tVec;
}
</code></pre>
<p>The strategically placed <code>prefetch</code> relies on CPU pipelining. The goal is to
have the CPU fetching the next chunk of data from RAM to cache while we are
calculating the current vector.</p>
<p>Lastly, we use <code>horizontal_add</code> to sum all values into a scalar value. Again, we
recognise Kahan's sum algorithm.</p>
<pre><code class="hljs css language-java"><span class="hljs-keyword">double</span> sum = horizontal_add(sumVec);
<span class="hljs-keyword">double</span> c = horizontal_add(cVec);
<span class="hljs-keyword">int</span> nans = horizontal_add(nancount);
<span class="hljs-keyword">for</span> (; d &lt; lim; d++) {
      <span class="hljs-keyword">double</span> x = *d;
    <span class="hljs-keyword">if</span> (x == x) {
        auto y = x - c;
        auto t = sum + y;
        c = (t - sum) -y;
        sum = t;
    } <span class="hljs-keyword">else</span> {
        nans++;
    }
}
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="comparison-with-clickhouse"></a><a href="#comparison-with-clickhouse" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Comparison with Clickhouse</h3>
<p>We compared how performance behaves when switching from naive (inaccurate) sum
to Kahan compensated sum.</p>
<h4><a class="anchor" aria-hidden="true" id="hardware"></a><a href="#hardware" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hardware</h4>
<p>We run all databases on an <code>c5.metal</code> AWS instance, which has two Intel 8275CL
24-core CPUs and 192GB of memory. QuestDB was running on 16 threads. As we
showed in a
<a href="2020-04-02-using-simd-to-aggregate-billions-of-rows-per-second.md">previous article</a>,
adding more threads does not improve performance beyond a certain point.
Clickhouse was running using all cores as per default configuration, however we
increased the memory limit from the default value from 10GB to 40GB
<code>&lt;max_memory_usage&gt;40000000000&lt;/max_memory_usage&gt;</code>.</p>
<h4><a class="anchor" aria-hidden="true" id="test-data"></a><a href="#test-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Test data</h4>
<p>We generated two test files using our
<a href="/docs/functionsRandomValueGenerators.md">random generation functions</a> and
exported the results to CSV. We then imported the CSV individually in the
databases.</p>
<pre><code class="hljs css language-sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-sql_function">rnd_double</span>() <span class="hljs-keyword">FROM</span> <span class="hljs-sql_function">long_sequence</span>(<span class="hljs-number">1</span>_000_000_000l); <span class="hljs-comment">-- non null
</span><span class="hljs-keyword">SELECT</span> <span class="hljs-sql_function">rnd_double</span>(<span class="hljs-number">2</span>) <span class="hljs-keyword">FROM</span> <span class="hljs-sql_function">long_sequence</span>(<span class="hljs-number">1</span>_000_000_000l); <span class="hljs-comment">-- with nulls
</span></code></pre>
<h4><a class="anchor" aria-hidden="true" id="storage-engine"></a><a href="#storage-engine" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Storage engine</h4>
<ul>
<li><strong>QuestDB</strong>: on disk</li>
<li><strong>Clickhouse</strong>: in memory (using the <code>memory()</code> engine)</li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="commands"></a><a href="#commands" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Commands</h4>
<h5><a class="anchor" aria-hidden="true" id="with-null"></a><a href="#with-null" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>With null</h5>
<table>
<thead>
<tr><th>Description</th><th>QuestDB</th><th>Clickhouse</th></tr>
</thead>
<tbody>
<tr><td>DDL</td><td><code>CREATE TABLE test_double AS(SELECT rnd_double() FROM long_sequence(1000000000L);</code></td><td><code>CREATE TABLE test_double (val Nullable(Float64)) Engine=Memory;</code></td></tr>
<tr><td>Import</td><td>Not required</td><td><code>clickhouse-client --query=&quot;INSERT INTO test_double FORMAT CSVWithNames;&quot; &lt; test_double.csv</code></td></tr>
<tr><td>Naive sum</td><td><code>SELECT sum(val) FROM test_double;</code></td><td><code>SELECT sum(val) FROM test_double;</code></td></tr>
<tr><td>Kahan sum</td><td><code>SELECT ksum(val) FROM test_double;</code></td><td><code>SELECT sumKahan(val) FROM test_double;</code></td></tr>
</tbody>
</table>
<h5><a class="anchor" aria-hidden="true" id="non-null"></a><a href="#non-null" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Non-null</h5>
<p>For non-null values, we adjusted the commands as follows</p>
<ul>
<li>use <code>test_double_not_nul.csv</code> instead of <code>test_double.csv</code>.</li>
<li>for Clickhouse, skip declaring val as <code>nullable</code>:
<code>CREATE TABLE test_double_not_null (val Float64) Engine=Memory;</code>.</li>
<li>for QuestDB, replace <code>rnd_double()</code> by <code>rnd_double(2)</code> at the DDL step.</li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="results"></a><a href="#results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h4>
<p>We ran each query several times for both QuestDB and Clickhouse and kept the
best result.</p>
<p>Without null values, both databases sum naively at roughly the same speed. With
Kahan summation, QuestDB performs at the same speed while Clickhouse's
performance drops by ~40%.</p>
<p><img src="/blog/assets/kahan-naive-not-null.png" alt="alt-text"></p>
<p>As we include null values, Clickhouse's performance degrades by 28% and 50% for
naive and Kahan summation, respectively.</p>
<p><img src="/blog/assets/kahan-naive-null.png" alt="alt-text"></p>
<h4><a class="anchor" aria-hidden="true" id="concluding-remarks"></a><a href="#concluding-remarks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Concluding remarks</h4>
<p>It is useful to stabilize aggregation with compensated sums. We learned that vector-based calculation produce different arithmetic errors compared to non-vector calcs. The way the aggregation is executed by multiple threads is not constant. This can cause results to be different from one SQL run to another, if the sum is accuracy naive. Through compensated sums, the results are consistent and more accurate.</p>
<p>It was also both interesting and surprising to be able to quantify the effect of prefetch on what is essentially sequential memory read.</p>
<p>We hope you enjoyed reading this. If you like what we do and where we are going  - please consider joining our community. Your support means a lot to us.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/04/02/using-simd-to-aggregate-billions-of-rows-per-second">Using SIMD to aggregate billions of rows per second</a></h1><p class="post-meta">April 2, 2020</p><div class="authorBlock"><p class="post-authorName"><a target="_blank" rel="noreferrer noopener">Tancrede Collard</a></p></div></header><article class="post-content"><div><span><p><a href="https://www.questdb.io/getstarted" target="_blank"><img class="banner-4-2" src="/blog/assets/banner-4-2.png" alt="drawing"/></a></p>
<p><a href="https://en.wikipedia.org/wiki/SIMD" target="_blank">SIMD instructions</a> are specific CPU instruction sets for arithmetic calculations that use synthetic parallelisation.
The parallelisation is synthetic because instead of spreading the work across CPU cores,
SIMD performs vector operations on multiple items using a <strong>single</strong> CPU instruction.
In practice, if you were to add 8 numbers together, SIMD does that in 1 operation instead of 8.
We get compounded performance improvements by combining SIMD with actual parallelisation and spanning the work across CPUs.</p>
<p>QuestDB 4.2 introduces SIMD instructions, which made our aggregations faster by 100x!
QuestDB is available <a href="https://github.com/questdb/questdb">open-source under Apache 2.0</a>. If you like what we do, please consider <b> <a href="https://github.com/questdb/questdb"> following us on Github and starring our project <img class="yellow-star" src="/img/star-yellow.svg"/></a></b></p>
<p>As of now, SIMD operations are available for non-keyed aggregation queries, such as
<code>select sum(value) from table</code>. In future releases, we will extend these to keyed aggregations, for example
<code>select key, sum(value) from table</code> (note the intentional omission of <code>GROUP BY</code>). This will also result in ultrafast
aggregation for time-bucketed queries using <code>SAMPLE BY</code>.</p>
<h3><a class="anchor" aria-hidden="true" id="how-fast-is-it"></a><a href="#how-fast-is-it" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How fast is it?</h3>
<p>We ran performance tests using 2 different CPUs: the <a href="https://ark.intel.com/content/www/us/en/ark/products/134899/intel-core-i7-8850h-processor-9m-cache-up-to-4-30-ghz.html">Intel 8850H</a>
and the <a href="https://www.amd.com/en/products/cpu/amd-ryzen-9-3900x">AMD Ryzen 3900X</a>. Both were running on 4 threads.</p>
<h4><a class="anchor" aria-hidden="true" id="queries"></a><a href="#queries" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Queries</h4>
<table>
<thead>
<tr><th>Test</th><th>Query</th></tr>
</thead>
<tbody>
<tr><td>sum of 1Bn doubles <br/> no nulls</td><td>create table zz as (select rnd_double() d from long_sequence(1000000000)); <br/> select sum(d) from zz;</td></tr>
<tr><td>sum of 1Bn ints</td><td>create table zz as (select rnd_int() i from long_sequence(1000000000)); <br/> select sum(i) from zz;</td></tr>
<tr><td>sum of 1Bn longs</td><td>create table zz as (select rnd_long() l from long_sequence(1000000000));<br/>select sum(l) from zz;</td></tr>
<tr><td>max of 1Bn doubles</td><td>create table zz as (select rnd_double() d from long_sequence(1000000000));<br/>select max(d) from zz;</td></tr>
<tr><td>max of 1Bn longs</td><td>create table zz as (select rnd_long() l from long_sequence(1000000000));<br/>select max(l) from zz;</td></tr>
</tbody>
</table>
<h4><a class="anchor" aria-hidden="true" id="results"></a><a href="#results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h4>
<p><img src="/blog/assets/bench-8850h.png" alt="alt-text"></p>
<p><img src="/blog/assets/bench-3900x.png" alt="alt-text"></p>
<p>The dataset producing the results shown above does not contain NULL values. Interestingly, when introducing nulls, QuestDB sum() query time is unchanged.
This can be tested by creating the table as follows.</p>
<table>
<thead>
<tr><th>Test</th><th>Query</th></tr>
</thead>
<tbody>
<tr><td>sum of 1Bn doubles <br/>(nulls)</td><td>create table zz as (select rnd_double(5) d from long_sequence(1000000000));<br/>select sum(d) from zz;</td></tr>
</tbody>
</table>
<h4><a class="anchor" aria-hidden="true" id="we-can-improve-this-performance-further"></a><a href="#we-can-improve-this-performance-further" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>We can improve this performance further</h4>
<p>Our approach is currently slightly more complicated as we convert each 32-bit integer to a 64-bit long to avoid overflow.
By removing this overhead and more, there is scope left to make our implementation faster in the future.</p>
<h3><a class="anchor" aria-hidden="true" id="perspectives-on-performance"></a><a href="#perspectives-on-performance" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Perspectives on performance</h3>
<p>The execution times outlined above become more interesting once put into context.
This is how QuestDB compares to Postgres when doing a sum of 1 billion numbers from a given table <code>select sum(d) from 1G_double_nonNull</code>.</p>
<p><img src="/blog/assets/bench-pg.png" alt="alt-text"></p>
<p>We found that our performance figures are constrained by the available memory channels. Both the 8850H and the 3900X
have 2 memory channels, and throwing more than 4 cores at the query above does not improve the performance.
On the other hand, if the CPU has more memory channels, then performance scales almost linearly.</p>
<p>To get an idea of the impact of memory channels, we spun off a m5.metal instance on AWS. This instance has two
24-core Intel 8275CL with 6 memory channels each. Here are the results compared to the 2-channel 3900X:</p>
<table>
<thead>
<tr><th>cpu cores</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th></tr>
</thead>
<tbody>
<tr><td>8275CL</td><td>910</td><td>605</td><td>380</td><td>240</td><td>193</td><td>176</td><td>156</td><td>148</td><td>140</td><td>136</td><td>133</td><td>141</td></tr>
<tr><td>3900X</td><td>621</td><td>502</td><td>381</td><td>260</td><td>260</td><td>260</td><td>260</td><td>260</td><td>260</td><td>260</td><td>260</td><td>260</td></tr>
</tbody>
</table>
<p>We plot those results below on the left. On the right-hand side, we normalise the results for each CPU and plot the performance
improvement of going from 1 to more cores.</p>
<p><img src="/blog/assets/core-scale.png" alt="alt-text"></p>
<p>Interestingly, the 2-channel 3900X, is much faster on 1 core than the
8275CL. But it does not scale well and hits a performance ceiling at 4 cores. This is because it only has 2 memory channels
that are already saturated. The 6-channel 8275CL allows QuestDB to
scale almost linearly as we add more CPU cores and hits a performance ceiling at around 12 cores.</p>
<p>Unfortunately AWS CPUs are hyperthreaded.
We could unpack even more performance if CPU were fully isolated to run the computations.</p>
<p>We did not get our hands on CPUs with more memory channels for this test, but if you have easy access to 8 or 12-channel servers and would like to benchmark QuestDB, we'd love to hear the results.
You can <a href="https://www.questdb.io/getstarted">download QuestDB</a> and leave a <a target="_blank" href="https://github.com/questdb/questdb/issues/146">comment on github</a></p>
<h3><a class="anchor" aria-hidden="true" id="what-is-next"></a><a href="#what-is-next" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What is next?</h3>
<p>In further releases, we will roll out this functionality to other parts of our SQL implementation. QuestDB implements SIMD in a generic fashion, which will allow us to continue adding SIMD to about everything our SQL engine does, such as keyed aggregations, indexing etc. We will also keep improving QuestDB's performance. Through some further work on assembly, we estimate that we can gain another 15% speed on these
operations. In the meantime, if you want to know exactly how we have achieved this, all of our code is <strong><a href="https://github.com/questdb/questdb">open-source</a></strong>!</p>
<h3><a class="anchor" aria-hidden="true" id="about-the-release-questdb-42"></a><a href="#about-the-release-questdb-42" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>About the release: QuestDB 4.2</h3>
<h4><a class="anchor" aria-hidden="true" id="summary"></a><a href="#summary" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Summary</h4>
<p>We have implemented SIMD-based vector execution of queries, such as <code>select sum(value) from table</code>.
This is ~100x faster than non-vector based execution. This is just the beginning as we will introduce vectors to more operations going forward.
Try our first implementation in this release - stay tuned for more features in the upcoming releases!</p>
<h4><a class="anchor" aria-hidden="true" id="important"></a><a href="#important" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Important</h4>
<p>Metadata file format has been changed to include a new flag for columns of type symbol.
It is necessary to convert existing tables to new format. Running the following sql: <code>repair table myTable</code> will update the table metadata.</p>
<h4><a class="anchor" aria-hidden="true" id="what-is-new"></a><a href="#what-is-new" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What is new?</h4>
<ul>
<li>Java: vectorized sum(), avg(), min(), max() for DOUBLE, LONG, INT</li>
<li>Java: select distinct symbol optimisation</li>
<li>FreeBSD support</li>
<li>Automatically restore data consistency and recover from partial data loss.</li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="what-we-fixed"></a><a href="#what-we-fixed" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What we fixed</h4>
<ul>
<li>SQL: NPE when parsing SQL text with malformed table name expression , for example ')', or ', blah'</li>
<li>SQL: parsing 'fill' clause in sub-query context was causing unexpected syntax error (#115)</li>
<li>SQL: possible internal error when ordering result of group-by or sample-by</li>
<li>Data Import: Ignore byte order marks (BOM) in table names created from an imported CSV (#114)</li>
<li>SQL: 'timestamp' propagation thru group-by code had issues. sum() was tripping over null values. Added last() aggregate function. (#113)</li>
<li>LOG: make service log names consistent on windows (#106)</li>
<li>SQL: deal with the following syntax 'select * from select ( select a from ....)'</li>
<li>SQL: allow the following syntax 'case cast(x as int) when 1 then ...'</li>
<li>fix(griffin): syntax check for &quot;case&quot;-')' overlap, e.g. &quot;a + (case when .. ) end&quot;</li>
</ul>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/03/15/interthread">The art of thread messaging</a></h1><p class="post-meta">March 15, 2020</p><div class="authorBlock"><p class="post-authorName"><a target="_blank" rel="noreferrer noopener">Vlad Ilyushchenko</a></p></div></header><article class="post-content"><div><span><h3><a class="anchor" aria-hidden="true" id="introduction"></a><a href="#introduction" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h3>
<p><img src="/blog/assets/threadmessaging.png" alt="drawing" width="480px"/></p>
<p>Inter-thread messaging is a fundamental part of any asynchronous system. It is the component responsible for transportation of data between threads. Messaging forms the infrastructure, the scaffolding of multi-threaded application and just like real-world transport infrastructure we want it to be inexpensive, fast, reliable and clean.</p>
<p>For QuestDB we wrote our own messaging and this post is about how it works and how fast it is.</p>
<div></div>
<h3><a class="anchor" aria-hidden="true" id="architecture"></a><a href="#architecture" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Architecture</h3>
<p>Borrowing heavily from world-famous Disruptor our messaging revolves around multiple threads accessing shared circular data structure. We call it RingQueue. Semantically RingQueue provides unbounded, index-based, random access to its elements. It does not coordinate concurrent access nor does it provide guarantees on thread safety. Coordination and thread-safety is a concern of Sequences. Sequences are responsible for providing indices that can be used to access RingQueue concurrently and safely.</p>
<p>To help sequences do their magic they have to be shaped into a graph. We start with syntax to chain sequences together:</p>
<p><code>a.then(b).then(c).then(d)</code></p>
<p>The result is a trivial sequence graph:</p>
<p><code>a -&gt; b -&gt; c -&gt; d</code></p>
<p>To branch we use helper class FanOut:</p>
<p><code>a.then(FanOut.to(b).and(c)).then(d)</code></p>
<p>The result is this sequence graph:</p>
<pre><code class="hljs css language-shell script">     +--&gt; B --&gt;+
A --&gt;|         |--&gt; D
     +--&gt; C --&gt;+
</code></pre>
<p>These two pieces of syntax are flexible enough to create any desired flow. This example shows that FanOut can have chain of sequences and other FanOuts:</p>
<p><code>a.then(FanOut.to(FanOut.to(b).and(c)).and(d.then(e)).then(f)</code></p>
<p>It is quite a mouthful but it creates this nice little graph:</p>
<pre><code class="hljs css language-shell script">        +--&gt; B --&gt;+
    +-&gt; |         |
    |   +--&gt; C --&gt;+
<span class="hljs-meta">A--&gt;</span><span class="bash">|             |--&gt; F </span>
    |             |
    +-&gt; D -&gt; E --&gt;+
</code></pre>
<p>FanOut can also be used as a placeholder in a chain to allow threads to subscribe/unsubscribe on the fly. Dynamic subscription is then simply adding a new sequence to FanOut:</p>
<pre><code class="hljs css language-java"><span class="hljs-comment">// You can add as many sequences into fan out as you like.</span>
<span class="hljs-comment">// Sequences can be added either up front or subscribe/unsubscribe on the fly.</span>
FanOut fanOut = <span class="hljs-keyword">new</span> FanOut();

<span class="hljs-comment">// ordinary producer sequence</span>
Sequence seqProducer = <span class="hljs-keyword">new</span> SPSequence(queue.getCapacity());
<span class="hljs-comment">// daisy chain producer and fan out and loop back producer</span>
seqProducer.then(fanOut).then(seqProducer);

<span class="hljs-comment">// meanwhile in another thread ....</span>
...

<span class="hljs-comment">// Add individual consumer sequences later as needed.</span>
<span class="hljs-comment">// This is thread safe non-blocking operation that can be performed from any thread.</span>
<span class="hljs-comment">// It is important to use current producer position as consumer starting point when subscribing on the fly.</span>
Sequence consumer1 = fanOut.addAndGet(<span class="hljs-keyword">new</span> SCSequence(seqProducer.current()));

<span class="hljs-comment">// do something useful with consumer1 sequence</span>
...

<span class="hljs-comment">// remove sequence from fanOut to unsubscribe</span>
fanOut.remove(consumer1);
</code></pre>
<p>Typical graph must contain single producer sequence and one or more consumer sequences. It will also have to be circular, e.g. to start and end with producer sequence. Graph has to be circular because we use circular underlying data structure, RingQueue. Without loop-back producer would be liable to overwrite queue elements before consumers had a chance to read them. Worse still, queue elements can be written to and read from concurrently. We don't want that to happen, right?</p>
<p>To help create practical sequence graph we implemented 4 types of sequences we can play with. These sequences are better understood as combination of their types and properties. SP - single producer, MP - multiple producer, SC - single consumer and MC - multiple consumer. Multi- sequences allow concurrent access and they guarantee that no two threads can retrieve same index. It is this property adds extra fun dimension to sequence graphs. Consider this graph:</p>
<p><code>A -&gt; B -&gt; A</code></p>
<p>or in Java notations:</p>
<p><code>A.then(B).then(A)</code></p>
<p>When &quot;B&quot; is an instance of MCSequence() we have a self-balancing worker pool. When &quot;A&quot; is MPSequence(), we have many-to-many pub-sub system. Cool, eh?</p>
<p>Single- sequences are faster but they are not thread-safe. They should be preferred for single-threaded consumer models.</p>
<p>Lets take a look at how threads interact with sequences. This is a typical example of publisher:</p>
<pre><code class="hljs css language-java"><span class="hljs-comment">// loop until there is work to do</span>
<span class="hljs-comment">// consumer thread may be able to rely on producer to</span>
<span class="hljs-comment">// publish "special" message to indicate end of stream.</span>
<span class="hljs-keyword">while</span> (<span class="hljs-keyword">true</span>) {
  
  <span class="hljs-comment">// Non-blocking call. Method returns immediately either with zero-based</span>
  <span class="hljs-comment">// ring queue index or negative long indicating one of following:</span>
  <span class="hljs-comment">// -1 = queue is empty</span>
  <span class="hljs-comment">// -2 = there was a contest for queue index and this thread has lost</span>
  <span class="hljs-keyword">long</span> cursor = sequence.next();
  <span class="hljs-keyword">if</span> (cursor &lt; <span class="hljs-number">0</span>) {
    <span class="hljs-comment">// negative cursor is an error</span>
    <span class="hljs-comment">// thread has a choice of things to do:</span>
    <span class="hljs-comment">// - busy spin</span>
    <span class="hljs-comment">// - yield/park</span>
    <span class="hljs-comment">// - work on something else</span>
    LockSupport.parkNanos(<span class="hljs-number">1</span>);
    <span class="hljs-keyword">continue</span>;
  }
  
  <span class="hljs-comment">// write to queue</span>
  <span class="hljs-keyword">try</span> {
    queue.get(cursor).value;
  } <span class="hljs-keyword">finally</span> {
    <span class="hljs-comment">// releasing cursor promptly is important</span>
    sequence.done(cursor);
  }
}

</code></pre>
<p><code>Sequence.next()</code> return values are:</p>
<p>-1  Queue is unavailable. It is either full or empty, depending on whether it is producer or consumer sequence</p>
<p>-2  Temporary race condition. Sequence failed CAS and delegated decision to your application.</p>
<p>Consumer sequence interaction is almost identical. The only difference would be consumer reading queue item instead of writing it.</p>
<p>Performance of single-threaded sequences can benefit further from batching. Batching relies on receiving range of indices from sequence and calling done() at end of batch rather than for every queue item. This is what consumer code might look like (producer code is the same):</p>
<pre><code class="hljs css language-java"><span class="hljs-keyword">while</span> (running) {
  <span class="hljs-keyword">long</span> cursor = sequence.next();
  
  <span class="hljs-keyword">if</span> (cursor &lt; <span class="hljs-number">0</span>) {
    LockSupport.parkNanos(<span class="hljs-number">1</span>);
    <span class="hljs-keyword">continue</span>;
  }

  <span class="hljs-comment">// get max index sequence can reach</span>
  <span class="hljs-keyword">long</span> available = sequence.available();
  
  <span class="hljs-comment">// look thru queue elements without using sequence</span>
  <span class="hljs-keyword">while</span> (cursor &lt; available) {
    queue.get(cursor++);
  }
  
  <span class="hljs-comment">// calling done() only once per batch can yield significant performance benefit</span>
  sequence.done(available - <span class="hljs-number">1</span>);
}
</code></pre>
<p>Multi-threaded sequence do not support batches.</p>
<h3><a class="anchor" aria-hidden="true" id="performance"></a><a href="#performance" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Performance</h3>
<p>I used Shipilev's project that already had Disruptor benchmark and I added QuestDB implementation of the same pipeline.</p>
<p>Benchmark source on <strong><a href="https://github.com/bluestreak01/disrupting-fjp">GitHub</a></strong></p>
<p><strong>2 CPU MBP 2015</strong></p>
<pre><code class="hljs css language-shell script">Benchmark          (slicesK)  (threads)  (workMult)  Mode  Cnt    Score    Error  Units
Disruptor.run            500          2          10    ss   50   10.043 ±  0.158  ms/op
Disruptor.run           1000          2          10    ss   50   19.944 ±  0.285  ms/op
Disruptor.run           5000          2          10    ss   50  133.082 ±  6.032  ms/op
QuestdbFanOut.run        500          2          10    ss   50   13.027 ±  0.180  ms/op
QuestdbFanOut.run       1000          2          10    ss   50   26.329 ±  0.327  ms/op
QuestdbFanOut.run       5000          2          10    ss   50  141.686 ±  4.129  ms/op
QuestdbWorker.run        500          2          10    ss   50   29.470 ±  0.976  ms/op
QuestdbWorker.run       1000          2          10    ss   50   62.205 ±  3.278  ms/op
QuestdbWorker.run       5000          2          10    ss   50  321.697 ± 12.031  ms/op
</code></pre>
<p><strong>4 CPU x5960 @ 4.2Ghz</strong></p>
<pre><code class="hljs css language-shell script">Benchmark          (slicesK)  (threads)  (workMult)  Mode  Cnt    Score    Error  Units
Disruptor.run            500          4          10    ss   50    6.892 ±  0.654  ms/op
Disruptor.run           1000          4          10    ss   50   10.143 ±  0.623  ms/op
Disruptor.run           5000          4          10    ss   50   54.084 ±  4.164  ms/op
QuestdbFanOut.run        500          4          10    ss   50    6.364 ±  0.197  ms/op
QuestdbFanOut.run       1000          4          10    ss   50   11.454 ±  0.754  ms/op
QuestdbFanOut.run       5000          4          10    ss   50   50.928 ±  3.264  ms/op
QuestdbWorker.run        500          4          10    ss   50   14.240 ±  1.341  ms/op
QuestdbWorker.run       1000          4          10    ss   50   27.246 ±  2.777  ms/op
QuestdbWorker.run       5000          4          10    ss   50  142.207 ± 15.157  ms/op
</code></pre>
<p>Disruptor and QuestDB perform essentially the same.</p>
<h3><a class="anchor" aria-hidden="true" id="how-to-get-it"></a><a href="#how-to-get-it" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How to get it</h3>
<p>Our messaging system is on Maven central as a part of QuestDB. Don't worry about package size though, QuestDB jar weighs in at 3.6MB and has no dependencies. Jump <strong><a href="https://github.com/questdb/questdb/releases">here</a></strong> for version reference.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2019/12/19/lineprot">What makes QuestDB faster than InfluxDB</a></h1><p class="post-meta">December 19, 2019</p><div class="authorBlock"><p class="post-authorName"><a target="_blank" rel="noreferrer noopener">Tancrede Collard</a></p></div></header><article class="post-content"><div><span><p>Our background is in low-latency trading. We are obsessed with performance and have always wanted to build the fastest tech out there.</p>
<p>But let’s keep the suspense for a minute and introduce ourselves first. QuestDB is an open-source SQL time-series database. InfluxDB is the current market leader in time-series, and we thought it would only be fair if we had a stab at their ingestion format called <strong>Influx line protocol (“ILP”)</strong> to compare data ingestion performance between QuestDB and InfluxDB.
It would not be an overstatement to say that InfluxDB uses a lot of CPU. We set ourselves to build a receiver for ILP, which stores data faster than InfluxDB while being hardware efficient.</p>
<p>We built QuestDB in zero-GC Java. Hopefully, the Java community will be proud!</p>
<h3><a class="anchor" aria-hidden="true" id="why-ilp"></a><a href="#why-ilp" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why ILP?</h3>
<p>Starting with QuestDB 4.0.4, users can ingest data through ILP to <strong>leverage SQL to query Influx data alongside other tables in a relational database while keeping the flexibility of ILP</strong>.</p>
<p><img src="/blog/assets/storeasmany.png" alt="alt-text"></p>
<blockquote>
<p>Store (fast) as many — query fast) as one.</p>
</blockquote>
<h3><a class="anchor" aria-hidden="true" id="data-loss-over-udp"></a><a href="#data-loss-over-udp" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data loss over UDP</h3>
<p>We have conducted our testing over UDP, thus expecting some level of data loss. However, we did not anticipate that InfluxDB would lose so much.</p>
<p>We have built a sender, which caches outgoing messages in a small buffer before sending them to a UDP socket. It sends data as fast as possible to eventually overpower the consumers and introduce packet loss. To test for different use cases, we have throttled the sender by varying the size of its buffer. A smaller buffer results in more frequent network calls and results in lower sending rates.</p>
<p>The benchmark publishes 50 million messages at various speeds. We then measure the number of entries in each DB after the fact to calculate the implied capture rate.</p>
<p>We use the Dell XPS 15 7590, 64Gb RAM, 6-core i9 CPU, 1TB SSD drive. In this experiment, both the sender and QuestDB/InfluxDB instance run on the same machine. UDP publishing is over loopback. OS is Fedora 31, OS UDP buffer size (net.core.rmem_max) is 104_857_600.</p>
<h3><a class="anchor" aria-hidden="true" id="it-comes-down-to-ingestion-speed"></a><a href="#it-comes-down-to-ingestion-speed" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>It comes down to ingestion speed</h3>
<p>Database performance is the bottleneck that results in packet loss. Messages are denied entry, and the loss rate is a direct function of the underlying database speed.</p>
<p>By sending 50m messages at different speeds, we get the following outcome.</p>
<p><img src="/blog/assets/capturerate.png" alt="alt-text"></p>
<blockquote>
<p>Capture rate as a function of sender speed</p>
</blockquote>
<p>InfluxDB’s capture rate rapidly drops below 50%, eventually converging toward single-digit rates.</p>
<p><img src="/blog/assets/captureratechart.png" alt="alt-text"></p>
<blockquote>
<p>Capture rate as a function of sending speed.</p>
</blockquote>
<p><img src="/blog/assets/impliedspeed.png" alt="alt-text"></p>
<blockquote>
<p>Implied ingestion speed in function of Sender speed</p>
</blockquote>
<p>QuestDB’s ingestion speed results are obtained through ILP. Our ingestion speed is considerably higher while using our native input formats instead.</p>
<h3><a class="anchor" aria-hidden="true" id="why-is-the-senders-rate-slower-for-influxdb-compared-to-questdb"></a><a href="#why-is-the-senders-rate-slower-for-influxdb-compared-to-questdb" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why is the sender’s rate slower for InfluxDB compared to QuestDB?</h3>
<p>In this test, we run the sender and the DB on the same machine, and it turns out that <strong>InfluxDB slows down our UDP sender by cannibalizing the CPU</strong>. Here is what happens to your CPUs while using InfluxDB:</p>
<p><img src="/blog/assets/cpuinflux.png" alt="alt-text"></p>
<blockquote>
<p>InfluxDB’s CPU usage when serving requests</p>
</blockquote>
<p>When in use, InfluxDB saturates all of the CPU. As a consequence, it slows down any other program running on the same machine.</p>
<h3><a class="anchor" aria-hidden="true" id="questdbs-secret-sauce"></a><a href="#questdbs-secret-sauce" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>QuestDB’s secret sauce</h3>
<p>We maximise the utilization of each CPU, from which we extract as much performance as possible. For the example below, we compared InfluxDB’s ingestion speed using 12 cores to QuestDB using one CPU core only. Despite utilizing one core instead of 12, QuestDB still outperforms InfluxDB significantly.</p>
<p>If spare CPU capacity arises, QuestDB will execute multiple data ingestions in parallel, leveraging multiple CPUs at the same time, but with one key difference; QuestDB uses work-stealing algorithms to ensure every last bit of CPU capacity is used while never being idle. Let us illustrate why this is the case.</p>
<p>Modern network cards have much superior throughput than the single receiver. Being limited to one receiver by design, InfluxDB considerably under-utilizes the network card, which is the limiting factor in the pipeline.
<img src="/blog/assets/queueinflux.png" alt="alt-text"></p>
<blockquote>
<p>All CPU cores open one single receiver that under-utilizes the network card</p>
</blockquote>
<p>Conversely, QuestDB can open parallel receivers (requiring one core each), fully utilizing the network card capabilities.
The following illustration assumes that there would be spare CPU capacity in other cores to be filled. In such a scenario we would get QuestDB utilizing 12 cores, with each one of those being considerably faster than InfluxDB’s combined 12 cores!</p>
<p><img src="/blog/assets/queuequest.png" alt="alt-text"></p>
<blockquote>
<p>Each CPU core opens an independent receiver working in parallel that fully leverages the network card</p>
</blockquote>
<p>Besides ingestion, InfluxDB also saturates the CPU on queries. The current user cannibalizes the whole CPU, while other users have to wait for their turn.
<img src="/blog/assets/userinflux.png" alt="alt-text"></p>
<blockquote>
<p>Users monopolize all CPU cores one after the other</p>
</blockquote>
<p>By contrast, QuestDB uses each core separately, allowing multiple users to query or write concurrently without delay. The performance gap between QuestDB and InfluxDB grows significantly as the number of simultaneous users increases.
<img src="/blog/assets/userquest.png" alt="alt-text"></p>
<blockquote>
<p>Users share CPU cores and are served concurrently, fast. They also use cores to the maximum.</p>
</blockquote>
<h3><a class="anchor" aria-hidden="true" id="get-started"></a><a href="#get-started" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Get started</h3>
<p>QuestDB supports ILP over UDP multicast and unicast sockets. TCP support will follow shortly. You don’t need to change anything in your application. For Telegraf, you can configure the UDP sender for QuestDB’s address and port.</p>
<p>Follow this link to <strong><a href="http://questdb.io/getstarted">download QuestDB</a></strong>. You can also use our <strong><a href="https://github.com/questdb/questdb/blob/master/benchmarks/src/main/java/org/questdb/LineUDPSenderMain.java">sender</a></strong> against QuestDB and InfluxDB to reproduce the experiment.</p>
<p>You can use JOINs while modifying your data structure on the fly and querying it all in SQL.</p>
</span></div></article></div><div class="docs-prevnext"></div></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><div align="left" class="footersection"><h5>QuestDB</h5><a href="/docs/documentationOverview">Documentation</a><a href="/getstarted">Download</a><a href="https://github.com/questdb/questdb/projects/2">Roadmap</a></div><div align="left" class="footersection"><h5>Community</h5><a href="https://join.slack.com/t/questdb/shared_invite/enQtNzk4Nzg4Mjc2MTE2LTEzZThjMzliMjUzMTBmYzVjYWNmM2UyNWJmNDdkMDYyZmE0ZDliZTQxN2EzNzk5MDE3Zjc1ZmJiZmFiZTIwMGY&gt;"> Join Slack </a><a href="https://twitter.com/" target="@QuestDB" rel="noreferrer noopener">Twitter</a></div><div align="left" class="footersection"><h5>More</h5><a href="/blog">Blog</a><a href="https://github.com/questdb/questdb/">GitHub</a><span class="sucker"><a class="github-button" href="https://github.com/questdb/questdb" data-icon="octicon-star" data-count-href="/questdb/questdb/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></span><span class="github-trigger"><a class="github-button" href="https://github.com/questdb/questdb" data-icon="octicon-star" data-count-href="/questdb/questdb/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></span></div></section><section class="copyright">Copyright © 2020 QuestDB</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
              !function(f,b,e,v,n,t,s)
              {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
              n.callMethod.apply(n,arguments):n.queue.push(arguments)};
              if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
              n.queue=[];t=b.createElement(e);t.async=!0;
              t.src=v;s=b.getElementsByTagName(e)[0];
              s.parentNode.insertBefore(t,s)}(window, document,'script',
              'https://connect.facebook.net/en_US/fbevents.js');
              fbq('init', '648273155994655');
              fbq('track', 'PageView');
                </script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: 'b2a69b4869a2a85284a82fb57519dcda',
                indexName: 'questdb',
                inputSelector: '#search_input_react',
                algoliaOptions: {}
              });
            </script></body></html>