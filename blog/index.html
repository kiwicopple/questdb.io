<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Blog · QuestDB</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Always on time"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Blog · QuestDB"/><meta property="og:type" content="website"/><meta property="og:url" content="https://questdb.io/"/><meta property="og:description" content="Always on time"/><meta property="og:image" content="https://questdb.io/img/favicon.png"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://questdb.io/img/favicon.png"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://questdb.io/blog/atom.xml" title="QuestDB Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://questdb.io/blog/feed.xml" title="QuestDB Blog RSS Feed"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-145747842-1', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600,700|Source+Code+Pro:400,700|Open+Sans:300,400,600,700"/><script type="text/javascript" src="/js/menu-hack.js"></script><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-block-buttons.js"></script><script type="text/javascript" src="/js/getstarted.js"></script><script type="text/javascript" src="/js/signup.js"></script><script type="text/javascript" src="/js/hotjar.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="blog"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/QuestDB_Logo.png" alt="QuestDB"/><h2 class="headerTitleWithLogo">QuestDB</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/" target="_self">Home</a></li><li class=""><a href="/getstarted" target="_self">Get QuestDB</a></li><li class=""><a href="/docs/documentationOverview" target="_self">Documentation</a></li><li class="siteNavGroupActive siteNavItemActive"><a href="/blog/" target="_self">Blog</a></li><li class=""><a href="/careers" target="_self">Careers</a></li><li class=""><a href="/about" target="_self">About</a></li><li class=""><a href="https://serieux-saucisson-79115.herokuapp.com/" target="_self">Join</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search..." title="Search..."/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Recent Posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Recent Posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/blog/2020/04/25/faster-more-accurate-sums">Faster more accurate sums</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/02/using-simd-to-aggregate-billions-of-rows-per-second">Using SIMD to aggregate billions of rows per second</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/15/interthread">The art of thread messaging</a></li><li class="navListItem"><a class="navItem" href="/blog/2019/12/19/lineprot">What makes QuestDB faster than InfluxDB</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="posts"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/04/25/faster-more-accurate-sums">Faster more accurate sums</a></h1><p class="post-meta">April 25, 2020</p><div class="authorBlock"><p class="post-authorName"><a target="_blank" rel="noreferrer noopener">Tancrede Collard</a></p></div></header><article class="post-content"><div><span><p><strong>Summing floating-point values has always implied a tradeoff between speed and accuracy. Until now.</strong></p>
<p><img src="/blog/assets/road-runner.png" alt="alt-text"></p>
<p>With <a href="https://github.com/questdb/questdb/releases/tag/4.2.1">release 4.2.1</a>, QuestDB implements Kahan and Neumaier compensated summation algorithms that perform as fast
as naive approaches. That's right! More accurate results in the same execution time.</p>
<p>You can find our code in our
<b> <a href="https://github.com/questdb/questdb" target="_blank"> Github repo. If you like it, please consider leaving a star :-) <img class="yellow-star" src="/img/star-yellow.svg"/></a></b></p>
<h3><a class="anchor" aria-hidden="true" id="tldr"></a><a href="#tldr" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TLDR</h3>
<p>In this post we cover</p>
<ul>
<li>An introductory <a href="#more-accurate-additions">example</a> of the problem with summing doubles.</li>
<li>An <a href="#float-representation">overview</a> of how floating-point numbers are represented.</li>
<li>A <a href="#kahans-algorithm-for-compensated-summation">presentation</a> of the Kahan algorithm.</li>
<li>Our <a href="#questdbs-implementation-with-avx-instructions">implementation</a> using SIMD instructions.</li>
<li>Performance <a href="#performance-impact-over-naive-method">benchmarks</a> between summation methods and <a href="#comparing-to-other-databases">across</a> some databases.</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="more-accurate-additions"></a><a href="#more-accurate-additions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>More accurate additions?</h3>
<p>Before we dig in, some of you might wonder how an addition can be inaccurate as opposed to simply right or wrong.</p>
<p>CPUs are poor at dealing with floating-point values. Arithmetics are almost always wrong, with a
worst-case error proportional to the number of operations <code>n</code>. As floating-point operations are intransitive, the order
in which you perform them also has an impact on accuracy. This is not because QuestDB uses a particular format for doubles.
In fact, it conforms with the <b> <a href="https://en.wikipedia.org/wiki/IEEE_754" target="_blank">IEEE standard</a></b>.</p>
<p>Here is an example:</p>
<pre><code class="hljs css language-java">    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>{
        System.out.println(<span class="hljs-number">5.1</span>+<span class="hljs-number">9.2</span>);
    }
</code></pre>
<p>We ask to add <code>5.1</code> to <code>9.2</code>. The result should be <code>14.3</code>, but we get the following.</p>
<pre><code class="hljs css language-shell script">14.299999999999999
</code></pre>
<p>It's a small (only <code>0.000000000000001</code>), but it is wrong. To make matters worse, this error can be compounded.</p>
<pre><code class="hljs css language-java">    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>{
        <span class="hljs-keyword">double</span> a = <span class="hljs-number">5.1</span>+<span class="hljs-number">9.2</span>;
        <span class="hljs-keyword">double</span> b = a + <span class="hljs-number">3.5</span>;
        <span class="hljs-keyword">double</span> c = <span class="hljs-number">14.3</span> + <span class="hljs-number">3.5</span>;
        System.out.println(<span class="hljs-string">"The result is: "</span> + b);
        System.out.print(<span class="hljs-string">"But we expected: "</span> + c);
    }
</code></pre>
<pre><code class="hljs css language-shell script">The result is: 17.799999999999997
But we expected: 17.8
</code></pre>
<p>The error has just grown to <code>0.000000000000003</code> and will keep on growing as we add operations.</p>
<h3><a class="anchor" aria-hidden="true" id="how-is-accuracy-lost"></a><a href="#how-is-accuracy-lost" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How is accuracy lost?</h3>
<h4><a class="anchor" aria-hidden="true" id="float-representation"></a><a href="#float-representation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Float representation</h4>
<p>To understand why, we first need to take a quick look at how floating-point numbers are represented in a computer.
Floats are stored in a format similar to scientific notation like the below.</p>
<p><img src="/blog/assets/float-eq-1.png" alt="alt-text"></p>
<p>However, the number of digits in the significand is fixed, and the base is <code>2</code> instead of <code>10</code>. In addition,
the exponent is a series of bits rather than a number. In practice, it looks more like the below.</p>
<p><img src="/blog/assets/float-eq-2.png" alt="alt-text"></p>
<p>If we simplify, it is basically the sum of <code>jx2^i</code> where <code>j</code> is either <code>0</code> or <code>1</code>.</p>
<p><img src="/blog/assets/float-eq-3.png" alt="alt-text"></p>
<p>To calculate a given number, you then multiply the significand by the base elevated to the exponent. As the
exponent is expressed in bits, it looks like the below. Note that we ignore inactivated bits and that bits reading
is done from right to left.</p>
<p><img src="/blog/assets/float-eq-4.png" alt="alt-text"></p>
<h4><a class="anchor" aria-hidden="true" id="what-about-negative-exponents"></a><a href="#what-about-negative-exponents" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What about negative exponents?</h4>
<p>It gets trickier when using negative exponents (i.e a right-side of the equation inferior to 1).
In this case, the right side consists of a sum of
negative powers of two. Here is an example of how this works. Note that in this case the bits in the exponent are read from
left to right.</p>
<p><img src="/blog/assets/float-eq-5.png" alt="alt-text"></p>
<p>Interestingly, this example shows one of the limits of floating-point representation. The design of the standard made
a trade-off and <strong>the format sacrifices precision for range</strong>. In the above example,
the <code>0011</code> pattern will repeat to infinity. Each new bit will get us closer to 0.1. But we will never exactly get there.
As a result, <strong>it is impossible to properly store <code>0.1</code>. Instead
computers store something really close, but not equal</strong>.</p>
<h4><a class="anchor" aria-hidden="true" id="truncation-accuracy-loss"></a><a href="#truncation-accuracy-loss" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Truncation accuracy loss</h4>
<p>As we saw above, floating-point numbers are not stored accurately. Naturally, operations these numbers will return
inaccurate results. That's not the only problem. Performing operations is also likely to introduce and grow errors
over time. One such case is when the result of an operation has to be truncated to fit the original format.
Here is a simplified example of the <strong>truncation</strong> that happens when adding floats of different orders of magnitude.</p>
<blockquote>
<p>For the below example we will be using base 10 and expressing the
exponent as a number rather than a binary for sake of simplicity. We will assume 5 significant digits.</p>
</blockquote>
<p>We start with both our numbers expressed in scientific notation.</p>
<p><img src="/blog/assets/sum-1.png" alt="alt-text"></p>
<p>Let's expand into decimal notation and place them on a similar scale so all digits fit.</p>
<p><img src="/blog/assets/sum-2.png" alt="alt-text"></p>
<p>Now, let's express this sum back as one number in scientific notation. We have to <code>truncate</code> the result back to 5 significant digits.</p>
<p><img src="/blog/assets/sum-3.png" alt="alt-text"></p>
<p>The result is completely off. In fact, it's as if we did not add anything.</p>
<h3><a class="anchor" aria-hidden="true" id="kahans-algorithm-for-compensated-summation"></a><a href="#kahans-algorithm-for-compensated-summation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Kahan's algorithm for compensated summation</h3>
<p>Compensated sum maintains a sum of accumulated errors and uses it to attempt to correct the (inaccurate) sum by the total error.
It does so by trying to adjust each new number by the total accumulated error.</p>
<p>The main Compensated summation algorithm is the <a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm" target="_blank">Kahan</a> sum. It runs in 4 steps:</p>
<ul>
<li>Subtract the <code>running error</code> from new <code>nunber</code> to get the <code>adjusted number</code>. If this is the first number, then the running error is 0.</li>
<li>Add the <code>adjusted number</code> to the <code>running total</code> and truncate to the number of significant digits. This is the <code>truncated result</code>.</li>
<li>Calculate the <code>new running error</code> as <code>(truncated result - running total) - adjusted number</code></li>
<li>Assign the <code>truncated result</code> as the new <code>running total</code>.</li>
</ul>
<p>Here is how Wikipedia describes the algorithm in pseudocode.</p>
<pre><code class="hljs css language-shell script">function KahanSum(input)
       var sum = 0.0                    // Prepare the accumulator.
       var c = 0.0                      // A running compensation for lost low-order bits.
       for i = 1 to input.length do     // The array input has elements indexed input[1] to input[input.length].
           var y = input[i] - c         // [step 1] c is zero the first time around.
           var t = sum + y              // [step 2] Alas, sum is big, y small, so low-order digits of y are lost.
           c = (t - sum) - y            // [step 3] (t - sum) cancels the high-order part of y; subtracting y recovers negative (low part of y)
           sum = t                      // [step 4] Algebraically, c should always be zero. Beware overly-aggressive optimizing compilers!
       next i                           // Next time around, the lost low part will be added to y in a fresh attempt.
       return sum
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="why-does-this-work"></a><a href="#why-does-this-work" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why does this work?</h3>
<p>Let's take a closer look at the following:</p>
<pre><code class="hljs css language-shell script">[1] c = (t - sum) - y
[2] t = sum + y
</code></pre>
<p>Let's replace <code>t</code> by <code>[2]</code> in our running error <code>c</code></p>
<pre><code class="hljs css language-shell script">c = (t - sum) - y
c = ((sum + y) - sum) - y
So applying addition transitivity to remove brackets, we obtain
c = sum + y - sum - y
c = 0       //???
</code></pre>
<p>With addition transitivity rules, the error should always be zero.<br>
Here is a quick example that illustrates how this is not the case. We assume 2 significant digits.</p>
<p>Initial values:</p>
<pre><code class="hljs css language-shell script">a = 0.2
b = 0.4
c = 12
</code></pre>
<p>Calculate <code>(a + b) + c</code>:</p>
<pre><code class="hljs css language-shell script">= (0.2 + 0.4) + 12
= (0.6) + 12        
= 13                // truncate to 2 significant digits and round up
</code></pre>
<p>Calculate <code>(a + c) + b</code>:</p>
<pre><code class="hljs css language-shell script">= (0.2 + 12) + 0.4
= 12 + 0.4          // truncate to 2 significant digits 
= 12                // truncate to 2 significant digits 
</code></pre>
<p>The non-transitivity is the reason why <code>((sum + y) - sum) - y</code> returns the error <code>c</code> instead of <code>0</code>.</p>
<h3><a class="anchor" aria-hidden="true" id="applying-the-algorithm-to-real-numbers"></a><a href="#applying-the-algorithm-to-real-numbers" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Applying the algorithm to real numbers</h3>
<p>Here is a great example from <a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm" target="_blank">Wikipedia</a> to illustrate the algorithm's cycle.</p>
<pre><code class="hljs css language-shell script">Assume initial error c is equal to 0
 y = 3.14159 - 0.00000             y = input[i] - c
  t = 10000.0 + 3.14159
    = 10003.14159                   But only six digits are retained.
    = 10003.1                       Many digits have been lost!
  c = (10003.1 - 10000.0) - 3.14159 This must be evaluated as written! 
    = 3.10000 - 3.14159             The assimilated part of y recovered, vs. the original full y.
    = -0.0415900                    Trailing zeros shown because this is six-digit arithmetic.
sum = 10003.1                       Thus, few digits from input(i) met those of sum.
</code></pre>
<p>The result is too large to be stored with full precision. On the next step, c gives the error.</p>
<pre><code class="hljs css language-shell script">  y = 2.71828 - (-0.0415900)        The shortfall from the previous stage gets included.
    = 2.75987                       It is of a size similar to y: most digits meet.
  t = 10003.1 + 2.75987             But few meet the digits of sum.
    = 10005.85987                   And the result is rounded
    = 10005.9                       To six digits.
  c = (10005.9 - 10003.1) - 2.75987 This extracts whatever went in.
    = 2.80000 - 2.75987             In this case, too much.
    = 0.040130                      But no matter, the excess would be subtracted off next time.
sum = 10005.9                       Exact result is 10005.85987, this is correctly rounded to 6 digits.
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="questdbs-implementation-with-avx-instructions"></a><a href="#questdbs-implementation-with-avx-instructions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>QuestDB's implementation with AVX instructions</h3>
<p>QuestDB implements the same 4-step algorithm as Kahan. However, it uses vectorised instructions to make this faster.
The idea came from Zach Bjornson who wrote about this on
<b> <a href="http://blog.zachbjornson.com/2019/08/11/fast-float-summation.html" target="_blank"> his blog</a></b>.</p>
<p>Although the implementations are the same, QuestDB's execution time differential between methods is radically different.
We dig into the details further along the article. Here is the implementation in QuestDB.</p>
<p>We first define our vectors</p>
<pre><code class="hljs css language-java">   Vec8d inputVec;
    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> step = <span class="hljs-number">8</span>;
    <span class="hljs-keyword">const</span> auto *lim = d + count;
    <span class="hljs-keyword">const</span> auto remainder = (int32_t) (count - (count / step) * step);
    <span class="hljs-keyword">const</span> auto *lim_vec = lim - remainder;
    Vec8d sumVec = <span class="hljs-number">0</span>.;
    Vec8d yVec;
    Vec8d cVec = <span class="hljs-number">0</span>.;
    Vec8db bVec;
    Vec8q nancount = <span class="hljs-number">0</span>;
    Vec8d tVec;
</code></pre>
<p>Then we load vectors with data. What's happening below is exactly Kahan's algorithm. However, instead of summing individual values,
we are summing vectors of 8 values each.</p>
<pre><code class="hljs css language-java">    <span class="hljs-keyword">for</span> (; d &lt; lim_vec; d += step) {
        _mm_prefetch(d + <span class="hljs-number">63</span> * step, _MM_HINT_T1);
        inputVec.load(d);
        bVec = is_nan(inputVec);
        nancount = if_add(bVec, nancount, <span class="hljs-number">1</span>);
        yVec = select(bVec, <span class="hljs-number">0</span>, inputVec - cVec);
        tVec = sumVec + yVec;
        cVec = (tVec - sumVec) - yVec;
        sumVec = tVec;
    }
</code></pre>
<p>You may have noted the <code>prefetch</code> instruction above. As QuestDB uses contiguous storage, we can use this instruction
to hint the CPU to fetch data ahead of time by loading the next page. Doing this means the CPU spends less time waiting for data.
Just this one trick improved our performance by around 20%.</p>
<p>Lastly, we use <code>horizontal_add</code> to sum all values into a scalar value. Again, we recognise Kahan's sum algorithm.</p>
<pre><code class="hljs css language-java">    <span class="hljs-keyword">double</span> sum = horizontal_add(sumVec);
    <span class="hljs-keyword">double</span> c = horizontal_add(cVec);
    <span class="hljs-keyword">int</span> nans = horizontal_add(nancount);
    <span class="hljs-keyword">for</span> (; d &lt; lim; d++) {
        <span class="hljs-keyword">double</span> x = *d;
        <span class="hljs-keyword">if</span> (x == x) {
            auto y = x - c;
            auto t = sum + y;
            c = (t - sum) -y;
            sum = t;
        } <span class="hljs-keyword">else</span> {
            nans++;
        }
    }
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="performance-impact-over-naive-method"></a><a href="#performance-impact-over-naive-method" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Performance impact over naive method</h3>
<p>The more accurate the result, the better. What is the price of increased accuracy?
Kahan algorithm adds each new <code>number</code> in 4 steps instead of one for the naive approach.
So in theory it should be 4x slower, right?</p>
<p><strong>In theory yes. Not with QuestDB.</strong></p>
<p>As we have shown in a
<a href="http://localhost:3000/blog/2020/04/02/using-simd-to-aggregate-billions-of-rows-per-second">previous article</a>, QuestDB's sum is actually so fast that it's bound by memory throughput. This means that
our CPU is actually spending a lot of time waiting for the memory.</p>
<p><img src="/blog/assets/thread-release.png" alt="alt-text"></p>
<p>What is QuestDB doing while it waits? Does it lock the CPU? Of course not! It releases it for someone else to use.
QuestDB's threading model is non-blocking. So whenever a thread is waiting, it does not stay idle and unavailable for other
work. It is released to the thread pool, and resumes work as soon as possible.</p>
<p>While we wait for the memory, <strong>we have plenty of CPU time which we can either release or use to improve sum accuracy without affecting performance</strong>. If we use kahan sum and
perform 4x as many operations, then as an illustration, our CPU usage time looks like this.</p>
<p><img src="/blog/assets/thread-work.png" alt="alt-text"></p>
<p><strong>We fill the gaps with extra work to get a more accurate result without affecting execution time.</strong></p>
<p>Here are the results when calculating the sum of 1 billion doubles on a quad-core i7.</p>
<p><img src="/blog/assets/bench-naive-kahan-neub.png" alt="alt-text"></p>
<p><strong>Granted, Kahan and Neumaier sum take slightly longer than the naive approach. However, the impact is a marginal 9%, not 400%!
Also, Neumaier's algo, which is a little more precise in certain edge cases but adds more operations to the algorithm performs just as
fast as Kahan for the same reason.</strong></p>
<h3><a class="anchor" aria-hidden="true" id="comparing-to-other-databases"></a><a href="#comparing-to-other-databases" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Comparing to other databases</h3>
<p>We compared how performance behaves when switching from naive (inaccurate) sum to kahan compensated sum. We run all
databases on an m5a.2xlarge AWS instance. We used QuestDB to generate a data file as follows and exported it as csv.</p>
<pre><code class="hljs css language-shell script">select rnd_double() from long_sequence(1_000_000_000);
</code></pre>
<table>
<thead>
<tr><th>Description</th><th>QuestDB</th><th>Clickhouse</th><th>kdb+</th></tr>
</thead>
<tbody>
<tr><td>File generation</td><td><code>select rnd_double() from long_sequence(1_000_000_000);</code></td><td></td><td></td></tr>
<tr><td>DDL</td><td><code>create table test_double(val double);</code></td><td><code>CREATE TABLE test_double (val Float64) Engine=Memory;</code></td><td>n/a</td></tr>
<tr><td>Import</td><td><code>insert into test_double 'test_double.csv';</code></td><td><code>clickhouse-client --query=&quot;INSERT INTO test_double FORMAT CSVWithNames;&quot; &lt; test_double.csv</code></td><td></td></tr>
<tr><td>Naive sum</td><td><code>select sum(val) from test_double</code></td><td><code>clickhouse-client --query=&quot;SELECT sum(val) FROM test_double</code></td><td><code>\t sum test_double</code></td></tr>
<tr><td>Kahan sum</td><td><code>select ksum(val) from test_double</code></td><td><code>clickhouse-client --query=&quot;SELECT sumKahan(val) FROM test_double</code></td><td>n/a</td></tr>
</tbody>
</table>
<p>The results are as below.</p>
<p><img src="/blog/assets/bench-kahan-kdb-clickhouse.png" alt="alt-text"></p>
<p>The three databases are summing naively at roughly the same speed. However, this summation method is inaccurate.
When using compensated sum (Kahan's algorithm), Clickhouse's performs twice as slow while QuestDB's performance only deteriorates by 9%.
kdb+ did not seem to offer compensated summation.</p>
<p>We are conscious this benchmark is not representative of any particular use case. We chose this test because
it is a basic query, and is trivial to reproduce. With more complex queries, we expect QuestDB's performance gap to compound.
But we still have a few features to implement before we can release more advanced benchmarks, hopefully in the next few weeks. So stay tuned!</p>
<h3><a class="anchor" aria-hidden="true" id="interested-in-performance"></a><a href="#interested-in-performance" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Interested in performance?</h3>
<p>If you are interested in what we are doing, want to solve performance issues (or get help to do so), feel free to drop by
our <b> <a href="https://serieux-saucisson-79115.herokuapp.com/" target="_blank">Slack channel</a></b>. We also post performance-geared
code on our <b> <a href="https://github.com/questdb/questdb" target="_blank"> Github </a></b> every day.
Our friendly community of high-performance junkies is growing every day, and we keep finding ways to pack more performance in.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/04/02/using-simd-to-aggregate-billions-of-rows-per-second">Using SIMD to aggregate billions of rows per second</a></h1><p class="post-meta">April 2, 2020</p><div class="authorBlock"><p class="post-authorName"><a target="_blank" rel="noreferrer noopener">Tancrede Collard</a></p></div></header><article class="post-content"><div><span><p><a href="https://www.questdb.io/getstarted" target="_blank"><img class="banner-4-2" src="/blog/assets/banner-4-2.png" alt="drawing"/></a></p>
<p><a href="https://en.wikipedia.org/wiki/SIMD" target="_blank">SIMD instructions</a> are specific CPU instruction sets for arithmetic calculations that use synthetic parallelisation.
The parallelisation is synthetic because instead of spreading the work across CPU cores,
SIMD performs vector operations on multiple items using a <strong>single</strong> CPU instruction.
In practice, if you were to add 8 numbers together, SIMD does that in 1 operation instead of 8.
We get compounded performance improvements by combining SIMD with actual parallelisation and spanning the work across CPUs.</p>
<p>QuestDB 4.2 introduces SIMD instructions, which made our aggregations faster by 100x!
QuestDB is available <a href="https://github.com/questdb/questdb">open-source under Apache 2.0</a>. If you like what we do, please consider <b> <a href="https://github.com/questdb/questdb"> following us on Github and starring our project <img class="yellow-star" src="/img/star-yellow.svg"/></a></b></p>
<p>As of now, SIMD operations are available for non-keyed aggregation queries, such as
<code>select sum(value) from table</code>. In future releases, we will extend these to keyed aggregations, for example
<code>select key, sum(value) from table</code> (note the intentional omission of <code>GROUP BY</code>). This will also result in ultrafast
aggregation for time-bucketed queries using <code>SAMPLE BY</code>.</p>
<h3><a class="anchor" aria-hidden="true" id="how-fast-is-it"></a><a href="#how-fast-is-it" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How fast is it?</h3>
<p>To get an idea of how fast aggregations have become, we ran a benchmark against kdb+, which is one of the fastest databases out there.
Coincidentally, their new version 4.0 (released a few days ago) introduces performance improvements through implicit parallelism.</p>
<h4><a class="anchor" aria-hidden="true" id="setup"></a><a href="#setup" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup</h4>
<p>We have benchmarked QuestDB against kdb's latest version using 2 different CPUs: the <a href="https://ark.intel.com/content/www/us/en/ark/products/134899/intel-core-i7-8850h-processor-9m-cache-up-to-4-30-ghz.html">Intel 8850H</a>
and the <a href="https://www.amd.com/en/products/cpu/amd-ryzen-9-3900x">AMD Ryzen 3900X</a>. Both databases were running on 4 threads.</p>
<h4><a class="anchor" aria-hidden="true" id="queries"></a><a href="#queries" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Queries</h4>
<table>
<thead>
<tr><th>Test</th><th>Query (kdb+ 4.0)</th><th>Query (QuestDB 4.2)</th></tr>
</thead>
<tbody>
<tr><td>sum of 1Bn doubles <br/> no nulls</td><td>zz:1000000000?1000.0 <br/>\t sum zz</td><td>create table zz as (select rnd_double() d from long_sequence(1000000000)); <br/> select sum(d) from zz;</td></tr>
<tr><td>sum of 1Bn ints</td><td>zz:1000000000?1000i <br/> \t sum zz</td><td>create table zz as (select rnd_int() i from long_sequence(1000000000)); <br/> select sum(i) from zz;</td></tr>
<tr><td>sum of 1Bn longs</td><td>zz:1000000000?1000j <br/>\t sum zz</td><td>create table zz as (select rnd_long() l from long_sequence(1000000000));<br/>select sum(l) from zz;</td></tr>
<tr><td>max of 1Bn doubles</td><td>zz:1000000000?1000.0<br/>\t max zz</td><td>create table zz as (select rnd_double() d from long_sequence(1000000000));<br/>select max(d) from zz;</td></tr>
<tr><td>max of 1Bn longs</td><td>zz:1000000000?1000<br/>\t max zz</td><td>create table zz as (select rnd_long() l from long_sequence(1000000000));<br/>select max(l) from zz;</td></tr>
</tbody>
</table>
<h4><a class="anchor" aria-hidden="true" id="results"></a><a href="#results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h4>
<p><img src="/blog/assets/bench-kdb-8850h.png" alt="alt-text"></p>
<p><img src="/blog/assets/bench-kdb-3900x.png" alt="alt-text"></p>
<p>The dataset producing the results shown above does not contain NULL values. Interestingly, as soon as the data contains NULL values, kdb+ sum() performance drops while QuestDB sum() query time is unchanged as seen on the chart below.</p>
<table>
<thead>
<tr><th>Test</th><th>Query (kdb+ 4.0)</th><th>Query (QuestDB 4.2)</th></tr>
</thead>
<tbody>
<tr><td>sum of 1Bn doubles <br/>(nulls)</td><td>zz:1000000000?1000.0 <br/>zz:?[zz&lt;100;0Nf;zz]<br/>\t sum zz</td><td>create table zz as (select rnd_double(5) d from long_sequence(1000000000));<br/>select sum(d) from zz;</td></tr>
</tbody>
</table>
<p><img src="/blog/assets/bench-kdb-8850H-sum-null.png" alt="alt-text"></p>
<h4><a class="anchor" aria-hidden="true" id="we-can-improve-this-performance-further"></a><a href="#we-can-improve-this-performance-further" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>We can improve this performance further</h4>
<p>QuestDB's sum(int) result is 64-bit long, whereas kdb+ sum(int) returns a 32-bit integer (even if the sum overflows).
Our approach is currently slightly more complicated as we convert each 32-bit integer to a 64-bit long to avoid overflow.
By removing this overhead and more, there is scope left to make our implementation faster in the future.</p>
<h3><a class="anchor" aria-hidden="true" id="perspectives-on-performance"></a><a href="#perspectives-on-performance" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Perspectives on performance</h3>
<p>The execution times outlined above become more interesting once put into context.
This is how QuestDB compares to Postgres when doing a sum of 1 billion numbers from a given table <code>select sum(d) from 1G_double_nonNull</code>.</p>
<p><img src="/blog/assets/bench-pg-kdb-quest.png" alt="alt-text"></p>
<p>We found that our performance figures are constrained by the available memory channels. Both the 8850H and the 3900X
have 2 memory channels, and throwing more than 4 cores at the query above does not improve the performance.
On the other hand, if the CPU has more memory channels, then performance scales almost linearly for both kdb+ and QuestDB.</p>
<p>To get an idea of the impact of memory channels, we spun off a m5.metal instance on AWS. This instance has two
24-core Intel 8275CL with 6 memory channels each. Here are the results compared to the 2-channel 3900X:</p>
<table>
<thead>
<tr><th>cpu cores</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th></tr>
</thead>
<tbody>
<tr><td>8275CL</td><td>910</td><td>605</td><td>380</td><td>240</td><td>193</td><td>176</td><td>156</td><td>148</td><td>140</td><td>136</td><td>133</td><td>141</td></tr>
<tr><td>3900X</td><td>621</td><td>502</td><td>381</td><td>260</td><td>260</td><td>260</td><td>260</td><td>260</td><td>260</td><td>260</td><td>260</td><td>260</td></tr>
</tbody>
</table>
<p>We plot those results below on the left. On the right-hand side, we normalise the results for each CPU and plot the performance
improvement of going from 1 to more cores.</p>
<p><img src="/blog/assets/core-scale.png" alt="alt-text"></p>
<p>Interestingly, the 2-channel 3900X, is much faster on 1 core than the
8275CL. But it does not scale well and hits a performance ceiling at 4 cores. This is because it only has 2 memory channels
that are already saturated. The 6-channel 8275CL allows QuestDB to
scale almost linearly as we add more CPU cores and hits a performance ceiling at around 12 cores.</p>
<p>Unfortunately AWS CPUs are hyperthreaded.
We could unpack even more performance if CPU were fully isolated to run the computations.</p>
<p>We did not get our hands on CPUs with more memory channels for this test, but if you have easy access to 8 or 12-channel servers and would like to benchmark QuestDB, we'd love to hear the results.
You can <a href="https://www.questdb.io/getstarted">download QuestDB</a> and leave a <a target="_blank" href="https://github.com/questdb/questdb/issues/146">comment on github</a></p>
<h3><a class="anchor" aria-hidden="true" id="what-is-next"></a><a href="#what-is-next" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What is next?</h3>
<p>In further releases, we will roll out this functionality to other parts of our SQL implementation. QuestDB implements SIMD in a generic fashion, which will allow us to continue adding SIMD to about everything our SQL engine does, such as keyed aggregations, indexing etc. We will also keep improving QuestDB's performance. Through some further work on assembly, we estimate that we can gain another 15% speed on these
operations. In the meantime, if you want to know exactly how we have achieved this, all of our code is <strong><a href="https://github.com/questdb/questdb">open-source</a></strong>!</p>
<h3><a class="anchor" aria-hidden="true" id="about-the-release-questdb-42"></a><a href="#about-the-release-questdb-42" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>About the release: QuestDB 4.2</h3>
<h4><a class="anchor" aria-hidden="true" id="summary"></a><a href="#summary" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Summary</h4>
<p>We have implemented SIMD-based vector execution of queries, such as <code>select sum(value) from table</code>.
This is ~100x faster than non-vector based execution. This is just the beginning as we will introduce vectors to more operations going forward.
Try our first implementation in this release - stay tuned for more features in the upcoming releases!</p>
<h4><a class="anchor" aria-hidden="true" id="important"></a><a href="#important" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Important</h4>
<p>Metadata file format has been changed to include a new flag for columns of type symbol.
It is necessary to convert existing tables to new format. Running the following sql: <code>repair table myTable</code> will update the table metadata.</p>
<h4><a class="anchor" aria-hidden="true" id="what-is-new"></a><a href="#what-is-new" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What is new?</h4>
<ul>
<li>Java: vectorized sum(), avg(), min(), max() for DOUBLE, LONG, INT</li>
<li>Java: select distinct symbol optimisation</li>
<li>FreeBSD support</li>
<li>Automatically restore data consistency and recover from partial data loss.</li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="what-we-fixed"></a><a href="#what-we-fixed" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What we fixed</h4>
<ul>
<li>SQL: NPE when parsing SQL text with malformed table name expression , for example ')', or ', blah'</li>
<li>SQL: parsing 'fill' clause in sub-query context was causing unexpected syntax error (#115)</li>
<li>SQL: possible internal error when ordering result of group-by or sample-by</li>
<li>Data Import: Ignore byte order marks (BOM) in table names created from an imported CSV (#114)</li>
<li>SQL: 'timestamp' propagation thru group-by code had issues. sum() was tripping over null values. Added last() aggregate function. (#113)</li>
<li>LOG: make service log names consistent on windows (#106)</li>
<li>SQL: deal with the following syntax 'select * from select ( select a from ....)'</li>
<li>SQL: allow the following syntax 'case cast(x as int) when 1 then ...'</li>
<li>fix(griffin): syntax check for &quot;case&quot;-')' overlap, e.g. &quot;a + (case when .. ) end&quot;</li>
</ul>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/03/15/interthread">The art of thread messaging</a></h1><p class="post-meta">March 15, 2020</p><div class="authorBlock"><p class="post-authorName"><a target="_blank" rel="noreferrer noopener">Vlad Ilyushchenko</a></p></div></header><article class="post-content"><div><span><h3><a class="anchor" aria-hidden="true" id="introduction"></a><a href="#introduction" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h3>
<p><img src="/blog/assets/threadmessaging.png" alt="drawing" width="480px"/></p>
<p>Inter-thread messaging is a fundamental part of any asynchronous system. It is the component responsible for transportation of data between threads. Messaging forms the infrastructure, the scaffolding of multi-threaded application and just like real-world transport infrastructure we want it to be inexpensive, fast, reliable and clean.</p>
<p>For QuestDB we wrote our own messaging and this post is about how it works and how fast it is.</p>
<div></div>
<h3><a class="anchor" aria-hidden="true" id="architecture"></a><a href="#architecture" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Architecture</h3>
<p>Borrowing heavily from world-famous Disruptor our messaging revolves around multiple threads accessing shared circular data structure. We call it RingQueue. Semantically RingQueue provides unbounded, index-based, random access to its elements. It does not coordinate concurrent access nor does it provide guarantees on thread safety. Coordination and thread-safety is a concern of Sequences. Sequences are responsible for providing indices that can be used to access RingQueue concurrently and safely.</p>
<p>To help sequences do their magic they have to be shaped into a graph. We start with syntax to chain sequences together:</p>
<p><code>a.then(b).then(c).then(d)</code></p>
<p>The result is a trivial sequence graph:</p>
<p><code>a -&gt; b -&gt; c -&gt; d</code></p>
<p>To branch we use helper class FanOut:</p>
<p><code>a.then(FanOut.to(b).and(c)).then(d)</code></p>
<p>The result is this sequence graph:</p>
<pre><code class="hljs css language-shell script">     +--&gt; B --&gt;+
A --&gt;|         |--&gt; D
     +--&gt; C --&gt;+
</code></pre>
<p>These two pieces of syntax are flexible enough to create any desired flow. This example shows that FanOut can have chain of sequences and other FanOuts:</p>
<p><code>a.then(FanOut.to(FanOut.to(b).and(c)).and(d.then(e)).then(f)</code></p>
<p>It is quite a mouthful but it creates this nice little graph:</p>
<pre><code class="hljs css language-shell script">        +--&gt; B --&gt;+
    +-&gt; |         |
    |   +--&gt; C --&gt;+
<span class="hljs-meta">A--&gt;</span><span class="bash">|             |--&gt; F </span>
    |             |
    +-&gt; D -&gt; E --&gt;+
</code></pre>
<p>FanOut can also be used as a placeholder in a chain to allow threads to subscribe/unsubscribe on the fly. Dynamic subscription is then simply adding a new sequence to FanOut:</p>
<pre><code class="hljs css language-java"><span class="hljs-comment">// You can add as many sequences into fan out as you like.</span>
<span class="hljs-comment">// Sequences can be added either up front or subscribe/unsubscribe on the fly.</span>
FanOut fanOut = <span class="hljs-keyword">new</span> FanOut();

<span class="hljs-comment">// ordinary producer sequence</span>
Sequence seqProducer = <span class="hljs-keyword">new</span> SPSequence(queue.getCapacity());
<span class="hljs-comment">// daisy chain producer and fan out and loop back producer</span>
seqProducer.then(fanOut).then(seqProducer);

<span class="hljs-comment">// meanwhile in another thread ....</span>
...

<span class="hljs-comment">// Add individual consumer sequences later as needed.</span>
<span class="hljs-comment">// This is thread safe non-blocking operation that can be performed from any thread.</span>
<span class="hljs-comment">// It is important to use current producer position as consumer starting point when subscribing on the fly.</span>
Sequence consumer1 = fanOut.addAndGet(<span class="hljs-keyword">new</span> SCSequence(seqProducer.current()));

<span class="hljs-comment">// do something useful with consumer1 sequence</span>
...

<span class="hljs-comment">// remove sequence from fanOut to unsubscribe</span>
fanOut.remove(consumer1);
</code></pre>
<p>Typical graph must contain single producer sequence and one or more consumer sequences. It will also have to be circular, e.g. to start and end with producer sequence. Graph has to be circular because we use circular underlying data structure, RingQueue. Without loop-back producer would be liable to overwrite queue elements before consumers had a chance to read them. Worse still, queue elements can be written to and read from concurrently. We don't want that to happen, right?</p>
<p>To help create practical sequence graph we implemented 4 types of sequences we can play with. These sequences are better understood as combination of their types and properties. SP - single producer, MP - multiple producer, SC - single consumer and MC - multiple consumer. Multi- sequences allow concurrent access and they guarantee that no two threads can retrieve same index. It is this property adds extra fun dimension to sequence graphs. Consider this graph:</p>
<p><code>A -&gt; B -&gt; A</code></p>
<p>or in Java notations:</p>
<p><code>A.then(B).then(A)</code></p>
<p>When &quot;B&quot; is an instance of MCSequence() we have a self-balancing worker pool. When &quot;A&quot; is MPSequence(), we have many-to-many pub-sub system. Cool, eh?</p>
<p>Single- sequences are faster but they are not thread-safe. They should be preferred for single-threaded consumer models.</p>
<p>Lets take a look at how threads interact with sequences. This is a typical example of publisher:</p>
<pre><code class="hljs css language-java"><span class="hljs-comment">// loop until there is work to do</span>
<span class="hljs-comment">// consumer thread may be able to rely on producer to</span>
<span class="hljs-comment">// publish "special" message to indicate end of stream.</span>
<span class="hljs-keyword">while</span> (<span class="hljs-keyword">true</span>) {
  
  <span class="hljs-comment">// Non-blocking call. Method returns immediately either with zero-based</span>
  <span class="hljs-comment">// ring queue index or negative long indicating one of following:</span>
  <span class="hljs-comment">// -1 = queue is empty</span>
  <span class="hljs-comment">// -2 = there was a contest for queue index and this thread has lost</span>
  <span class="hljs-keyword">long</span> cursor = sequence.next();
  <span class="hljs-keyword">if</span> (cursor &lt; <span class="hljs-number">0</span>) {
    <span class="hljs-comment">// negative cursor is an error</span>
    <span class="hljs-comment">// thread has a choice of things to do:</span>
    <span class="hljs-comment">// - busy spin</span>
    <span class="hljs-comment">// - yield/park</span>
    <span class="hljs-comment">// - work on something else</span>
    LockSupport.parkNanos(<span class="hljs-number">1</span>);
    <span class="hljs-keyword">continue</span>;
  }
  
  <span class="hljs-comment">// write to queue</span>
  <span class="hljs-keyword">try</span> {
    queue.get(cursor).value;
  } <span class="hljs-keyword">finally</span> {
    <span class="hljs-comment">// releasing cursor promptly is important</span>
    sequence.done(cursor);
  }
}

</code></pre>
<p><code>Sequence.next()</code> return values are:</p>
<p>-1  Queue is unavailable. It is either full or empty, depending on whether it is producer or consumer sequence</p>
<p>-2  Temporary race condition. Sequence failed CAS and delegated decision to your application.</p>
<p>Consumer sequence interaction is almost identical. The only difference would be consumer reading queue item instead of writing it.</p>
<p>Performance of single-threaded sequences can benefit further from batching. Batching relies on receiving range of indices from sequence and calling done() at end of batch rather than for every queue item. This is what consumer code might look like (producer code is the same):</p>
<pre><code class="hljs css language-java"><span class="hljs-keyword">while</span> (running) {
  <span class="hljs-keyword">long</span> cursor = sequence.next();
  
  <span class="hljs-keyword">if</span> (cursor &lt; <span class="hljs-number">0</span>) {
    LockSupport.parkNanos(<span class="hljs-number">1</span>);
    <span class="hljs-keyword">continue</span>;
  }

  <span class="hljs-comment">// get max index sequence can reach</span>
  <span class="hljs-keyword">long</span> available = sequence.available();
  
  <span class="hljs-comment">// look thru queue elements without using sequence</span>
  <span class="hljs-keyword">while</span> (cursor &lt; available) {
    queue.get(cursor++);
  }
  
  <span class="hljs-comment">// calling done() only once per batch can yield significant performance benefit</span>
  sequence.done(available - <span class="hljs-number">1</span>);
}
</code></pre>
<p>Multi-threaded sequence do not support batches.</p>
<h3><a class="anchor" aria-hidden="true" id="performance"></a><a href="#performance" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Performance</h3>
<p>I used Shipilev's project that already had Disruptor benchmark and I added QuestDB implementation of the same pipeline.</p>
<p>Benchmark source on <strong><a href="https://github.com/bluestreak01/disrupting-fjp">GitHub</a></strong></p>
<p><strong>2 CPU MBP 2015</strong></p>
<pre><code class="hljs css language-shell script">Benchmark          (slicesK)  (threads)  (workMult)  Mode  Cnt    Score    Error  Units
Disruptor.run            500          2          10    ss   50   10.043 ±  0.158  ms/op
Disruptor.run           1000          2          10    ss   50   19.944 ±  0.285  ms/op
Disruptor.run           5000          2          10    ss   50  133.082 ±  6.032  ms/op
QuestdbFanOut.run        500          2          10    ss   50   13.027 ±  0.180  ms/op
QuestdbFanOut.run       1000          2          10    ss   50   26.329 ±  0.327  ms/op
QuestdbFanOut.run       5000          2          10    ss   50  141.686 ±  4.129  ms/op
QuestdbWorker.run        500          2          10    ss   50   29.470 ±  0.976  ms/op
QuestdbWorker.run       1000          2          10    ss   50   62.205 ±  3.278  ms/op
QuestdbWorker.run       5000          2          10    ss   50  321.697 ± 12.031  ms/op
</code></pre>
<p><strong>4 CPU x5960 @ 4.2Ghz</strong></p>
<pre><code class="hljs css language-shell script">Benchmark          (slicesK)  (threads)  (workMult)  Mode  Cnt    Score    Error  Units
Disruptor.run            500          4          10    ss   50    6.892 ±  0.654  ms/op
Disruptor.run           1000          4          10    ss   50   10.143 ±  0.623  ms/op
Disruptor.run           5000          4          10    ss   50   54.084 ±  4.164  ms/op
QuestdbFanOut.run        500          4          10    ss   50    6.364 ±  0.197  ms/op
QuestdbFanOut.run       1000          4          10    ss   50   11.454 ±  0.754  ms/op
QuestdbFanOut.run       5000          4          10    ss   50   50.928 ±  3.264  ms/op
QuestdbWorker.run        500          4          10    ss   50   14.240 ±  1.341  ms/op
QuestdbWorker.run       1000          4          10    ss   50   27.246 ±  2.777  ms/op
QuestdbWorker.run       5000          4          10    ss   50  142.207 ± 15.157  ms/op
</code></pre>
<p>Disruptor and QuestDB perform essentially the same.</p>
<h3><a class="anchor" aria-hidden="true" id="how-to-get-it"></a><a href="#how-to-get-it" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How to get it</h3>
<p>Our messaging system is on Maven central as a part of QuestDB. Don't worry about package size though, QuestDB jar weighs in at 3.6MB and has no dependencies. Jump <strong><a href="https://github.com/questdb/questdb/releases">here</a></strong> for version reference.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2019/12/19/lineprot">What makes QuestDB faster than InfluxDB</a></h1><p class="post-meta">December 19, 2019</p><div class="authorBlock"><p class="post-authorName"><a target="_blank" rel="noreferrer noopener">Tancrede Collard</a></p></div></header><article class="post-content"><div><span><p>Our background is in low-latency trading. We are obsessed with performance and have always wanted to build the fastest tech out there.</p>
<p>But let’s keep the suspense for a minute and introduce ourselves first. QuestDB is an open-source SQL time-series database. InfluxDB is the current market leader in time-series, and we thought it would only be fair if we had a stab at their ingestion format called <strong>Influx line protocol (“ILP”)</strong> to compare data ingestion performance between QuestDB and InfluxDB.
It would not be an overstatement to say that InfluxDB uses a lot of CPU. We set ourselves to build a receiver for ILP, which stores data faster than InfluxDB while being hardware efficient.</p>
<p>We built QuestDB in zero-GC Java. Hopefully, the Java community will be proud!</p>
<h3><a class="anchor" aria-hidden="true" id="why-ilp"></a><a href="#why-ilp" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why ILP?</h3>
<p>Starting with QuestDB 4.0.4, users can ingest data through ILP to <strong>leverage SQL to query Influx data alongside other tables in a relational database while keeping the flexibility of ILP</strong>.</p>
<p><img src="/blog/assets/storeasmany.png" alt="alt-text"></p>
<blockquote>
<p>Store (fast) as many — query fast) as one.</p>
</blockquote>
<h3><a class="anchor" aria-hidden="true" id="data-loss-over-udp"></a><a href="#data-loss-over-udp" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data loss over UDP</h3>
<p>We have conducted our testing over UDP, thus expecting some level of data loss. However, we did not anticipate that InfluxDB would lose so much.</p>
<p>We have built a sender, which caches outgoing messages in a small buffer before sending them to a UDP socket. It sends data as fast as possible to eventually overpower the consumers and introduce packet loss. To test for different use cases, we have throttled the sender by varying the size of its buffer. A smaller buffer results in more frequent network calls and results in lower sending rates.</p>
<p>The benchmark publishes 50 million messages at various speeds. We then measure the number of entries in each DB after the fact to calculate the implied capture rate.</p>
<p>We use the Dell XPS 15 7590, 64Gb RAM, 6-core i9 CPU, 1TB SSD drive. In this experiment, both the sender and QuestDB/InfluxDB instance run on the same machine. UDP publishing is over loopback. OS is Fedora 31, OS UDP buffer size (net.core.rmem_max) is 104_857_600.</p>
<h3><a class="anchor" aria-hidden="true" id="it-comes-down-to-ingestion-speed"></a><a href="#it-comes-down-to-ingestion-speed" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>It comes down to ingestion speed</h3>
<p>Database performance is the bottleneck that results in packet loss. Messages are denied entry, and the loss rate is a direct function of the underlying database speed.</p>
<p>By sending 50m messages at different speeds, we get the following outcome.</p>
<p><img src="/blog/assets/capturerate.png" alt="alt-text"></p>
<blockquote>
<p>Capture rate as a function of sender speed</p>
</blockquote>
<p>InfluxDB’s capture rate rapidly drops below 50%, eventually converging toward single-digit rates.</p>
<p><img src="/blog/assets/captureratechart.png" alt="alt-text"></p>
<blockquote>
<p>Capture rate as a function of sending speed.</p>
</blockquote>
<p><img src="/blog/assets/impliedspeed.png" alt="alt-text"></p>
<blockquote>
<p>Implied ingestion speed in function of Sender speed</p>
</blockquote>
<p>QuestDB’s ingestion speed results are obtained through ILP. Our ingestion speed is considerably higher while using our native input formats instead.</p>
<h3><a class="anchor" aria-hidden="true" id="why-is-the-senders-rate-slower-for-influxdb-compared-to-questdb"></a><a href="#why-is-the-senders-rate-slower-for-influxdb-compared-to-questdb" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why is the sender’s rate slower for InfluxDB compared to QuestDB?</h3>
<p>In this test, we run the sender and the DB on the same machine, and it turns out that <strong>InfluxDB slows down our UDP sender by cannibalizing the CPU</strong>. Here is what happens to your CPUs while using InfluxDB:</p>
<p><img src="/blog/assets/cpuinflux.png" alt="alt-text"></p>
<blockquote>
<p>InfluxDB’s CPU usage when serving requests</p>
</blockquote>
<p>When in use, InfluxDB saturates all of the CPU. As a consequence, it slows down any other program running on the same machine.</p>
<h3><a class="anchor" aria-hidden="true" id="questdbs-secret-sauce"></a><a href="#questdbs-secret-sauce" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>QuestDB’s secret sauce</h3>
<p>We maximise the utilization of each CPU, from which we extract as much performance as possible. For the example below, we compared InfluxDB’s ingestion speed using 12 cores to QuestDB using one CPU core only. Despite utilizing one core instead of 12, QuestDB still outperforms InfluxDB significantly.</p>
<p>If spare CPU capacity arises, QuestDB will execute multiple data ingestions in parallel, leveraging multiple CPUs at the same time, but with one key difference; QuestDB uses work-stealing algorithms to ensure every last bit of CPU capacity is used while never being idle. Let us illustrate why this is the case.</p>
<p>Modern network cards have much superior throughput than the single receiver. Being limited to one receiver by design, InfluxDB considerably under-utilizes the network card, which is the limiting factor in the pipeline.
<img src="/blog/assets/queueinflux.png" alt="alt-text"></p>
<blockquote>
<p>All CPU cores open one single receiver that under-utilizes the network card</p>
</blockquote>
<p>Conversely, QuestDB can open parallel receivers (requiring one core each), fully utilizing the network card capabilities.
The following illustration assumes that there would be spare CPU capacity in other cores to be filled. In such a scenario we would get QuestDB utilizing 12 cores, with each one of those being considerably faster than InfluxDB’s combined 12 cores!</p>
<p><img src="/blog/assets/queuequest.png" alt="alt-text"></p>
<blockquote>
<p>Each CPU core opens an independent receiver working in parallel that fully leverages the network card</p>
</blockquote>
<p>Besides ingestion, InfluxDB also saturates the CPU on queries. The current user cannibalizes the whole CPU, while other users have to wait for their turn.
<img src="/blog/assets/userinflux.png" alt="alt-text"></p>
<blockquote>
<p>Users monopolize all CPU cores one after the other</p>
</blockquote>
<p>By contrast, QuestDB uses each core separately, allowing multiple users to query or write concurrently without delay. The performance gap between QuestDB and InfluxDB grows significantly as the number of simultaneous users increases.
<img src="/blog/assets/userquest.png" alt="alt-text"></p>
<blockquote>
<p>Users share CPU cores and are served concurrently, fast. They also use cores to the maximum.</p>
</blockquote>
<h3><a class="anchor" aria-hidden="true" id="get-started"></a><a href="#get-started" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Get started</h3>
<p>QuestDB supports ILP over UDP multicast and unicast sockets. TCP support will follow shortly. You don’t need to change anything in your application. For Telegraf, you can configure the UDP sender for QuestDB’s address and port.</p>
<p>Follow this link to <strong><a href="http://questdb.io/getstarted">download QuestDB</a></strong>. You can also use our <strong><a href="https://github.com/questdb/questdb/blob/master/benchmarks/src/main/java/org/questdb/LineUDPSenderMain.java">sender</a></strong> against QuestDB and InfluxDB to reproduce the experiment.</p>
<p>You can use JOINs while modifying your data structure on the fly and querying it all in SQL.</p>
</span></div></article></div><div class="docs-prevnext"></div></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><div align="left" class="footersection"><h5>QuestDB</h5><a href="/docs/documentationOverview">Documentation</a><a href="/getstarted">Download</a><a href="https://github.com/questdb/questdb/projects/2">Roadmap</a></div><div align="left" class="footersection"><h5>Community</h5><a href="https://stackoverflow.com/questions/tagged/" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://join.slack.com/t/questdb/shared_invite/enQtNzk4Nzg4Mjc2MTE2LTEzZThjMzliMjUzMTBmYzVjYWNmM2UyNWJmNDdkMDYyZmE0ZDliZTQxN2EzNzk5MDE3Zjc1ZmJiZmFiZTIwMGY&gt;"> Join Slack </a><a href="https://twitter.com/" target="@QuestDB" rel="noreferrer noopener">Twitter</a></div><div align="left" class="footersection"><h5>More</h5><a href="/blog">Blog</a><a href="https://github.com/questdb/questdb/">GitHub</a><span class="sucker"><a class="github-button" href="https://github.com/questdb/questdb" data-icon="octicon-star" data-count-href="/questdb/questdb/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></span><a class="github-button" href="https://github.com/questdb/questdb" data-icon="octicon-star" data-count-href="/questdb/questdb/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><section class="copyright">Copyright © 2020 QuestDB</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
              !function(f,b,e,v,n,t,s)
              {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
              n.callMethod.apply(n,arguments):n.queue.push(arguments)};
              if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
              n.queue=[];t=b.createElement(e);t.async=!0;
              t.src=v;s=b.getElementsByTagName(e)[0];
              s.parentNode.insertBefore(t,s)}(window, document,'script',
              'https://connect.facebook.net/en_US/fbevents.js');
              fbq('init', '648273155994655');
              fbq('track', 'PageView');
                </script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: 'b2a69b4869a2a85284a82fb57519dcda',
                indexName: 'questdb',
                inputSelector: '#search_input_react',
                algoliaOptions: {}
              });
            </script></body></html>